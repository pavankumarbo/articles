- In memory processing for large datasets
- RDD API- spark core
- SPark SQL - library for processing structured and semi structured data
- SPark MLLIB - for machine learning algorithms
- Spark streaming

- spark context - entry point where control is transferring from OS to program.
- sc.version - 2.3.1
- sc.PythonVer - 3.7
- sc.master
- sc.parallelize([])
- sc.textFile([])


- lambda - anonymous functions . useful in map and filter. only one expression is contained and is returned ex: lambda x : x+1
- map(lambda func, list) - appliest lambda func to all list items and list(map) gives a new list
- filter(lmabda func, list)- applies filter func to all items in the list and a new list is returned.


- RDD - resilient(resistant to failure), distributed(over multiple nodes and executors) , dataset(set, arrays,tuples etc)
- load from hdfs, s3, text file etc
- patrtition - logical division of dataset. by default spark creates partition based on available nodes.
- sc.textFile(minPartitions = ) . df.getNumPartitions() for getting partititons


- rdd.map() - transform each element using the lambda func and return new rdd
- rdd.filter()- filter each element using the lambda func and return new rdd
- rdd.flatMap()- transform multiple data sets and return single iterator
- rdd.take(n) - return n elements from the rdd
- rdd.collect()- return all elements from the rdd
- rdd1.union(rdd2).collect() - union two rdds


- pairrdds- create by returning tuple of values.containes key and value but all same as rdd
- pairRDD.reduceByKey(lambda x,y: x+y) - for matching keys returns tuple (key,sum of all x and y for that key)
- parRdd.sortByKey()- 
- pairRdd.groupByKey().collect() - return (key, list of values for that key)
- pairRdd1.join(pairRdd2).collect() - join based on key - returns (key, tuple of values for that key) 

- rdd.saveAsTextFile() - use instead of colelct,because collect takes lot of memory.
-- rdd.coalesce(1).saveAsTextFile() - returns the rdd
- countAsKey()- use only on small datasets
- collectAsMap() - returns key value paris in pairrdd as a dictionary, use it only on small dataset

- dataframe - immutable distributed colelction of structured(sql tables) and semistructired data(json etc). provided by sql api, sql queries can be written.
- spark.read - for reading into a dataframe - spark.read.csv('filepath',header=True,inferSchema=True)


- dataframe manipulation contains transformations and actions
- transformations - select, filter, groupby,orderby,dropDuplicates(), withColumnRenamed()
- actions - printSchema,head,count,columns, describe(), 
- .printschema() - prints the datatypes of columns
- .columns() - prints all column names
- .describe() is used to calculate summary statistics of all numerical attributes
- .count() - counts number of rows
- .dropDuplicates() - to drop dupliocate rows()

- df.createOrReplaceTempView("people")
- spark.sql('select * from people')


- handyspark for data visualization
- spark_df.toPandas()


MLLIB
- Collaborative filtering - ALS algorithm
- Clustering - 
- Classification - 

- rdd.rqandomSplit()
- LabeledVector()
- DenseVector()
- SparseVector()
- hashingTF() - term frequency



DATA CLEANING:
- clean for performance , organized data flow
- StructType([StructField('name',StringType(), True), StructField('name',IntegeraType(), True)]).  (import * from pyspark.sql.types)
- spark objects are immutable. meaning defined once, unable to be directly modified, component of functional programming, recreated if reassigned.
- aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')
- issues with csv- schema is not well defined, escape characters, encoding, cannoit be filteresd using predicate pushdown
- paraquet - schema stored internally,columnar data format, support predicate pushdown
- df.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')
- spark.read.parquet
- 
